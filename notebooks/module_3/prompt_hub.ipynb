{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"coding-assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'coding-assistant', 'lc_hub_commit_hash': '3351ed5c3b03d705b4a83a906ae712e422f8909b234cc61899df4cea5d0d0425'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You Are Coding expert of {language} language, answer all questions of that language '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'Answers', 'description': \"Extract answers from the LLM's response.\", 'type': 'object', 'properties': {'Answer': {'type': 'string', 'description': 'answer from llm'}}, 'required': ['Answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You Are Coding expert of Python language, answer all questions of that language ', additional_kwargs={}, response_metadata={}), HumanMessage(content='How We do file handling in python ??', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"How We do file handling in python ??\", \"language\": \"Python\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Ap8dYYJLCd27ip81LCvUGz4AToF04', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"File handling in Python can be performed using built-in functions and methods. The primary operations you can perform on files include reading from files, writing to files, and closing files. Here's a comprehensive overview of how to handle files in Python:\\n\\n### Opening a File\\n\\nYou can open a file using the `open()` function, which requires the file name and mode (read, write, etc.) as arguments:\\n\\n```python\\nfile = open('filename.txt', 'r')  # Open in read mode\\n```\\n\\nCommon modes:\\n- `'r'`: Read (default). Opens a file for reading, error if the file does not exist.\\n- `'w'`: Write. Opens a file for writing, creates a new file if it doesn't exist or truncates the file if it does.\\n- `'a'`: Append. Opens a file for appending, creates a new file if it doesn't exist.\\n- `'b'`: Binary mode (e.g., `'rb'`, `'wb'` for reading/writing binary files).\\n- `'x'`: Exclusive creation. If the file already exists, the operation will fail.\\n- `'t'`: Text mode (default).\\n\\n### Reading from a File\\n\\nYou can read the content of a file using various methods:\\n\\n```python\\n# Reading the entire file\\ncontent = file.read()\\n\\n# Reading line by line\\nline = file.readline()  # Reads the next line\\nlines = file.readlines()  # Reads all lines into a list\\n\\n# Use a loop to iterate through the file line by line\\nfor line in file:\\n    print(line)\\n```\\n\\n### Writing to a File\\n\\nYou can write to a file using the `write()` or `writelines()` methods:\\n\\n```python\\n# Opening a file in write mode\\nfile = open('filename.txt', 'w')\\n\\n# Writing a string to the file\\nfile.write('Hello, World!')\\n\\n# Writing multiple lines\\nlines = ['First line\\\\n', 'Second line\\\\n', 'Third line\\\\n']\\nfile.writelines(lines)\\n```\\n\\n### Closing a File\\n\\nOnce you're done with file operations, it's important to close the file to free up resources:\\n\\n```python\\nfile.close()\\n```\\n\\n### Using Context Manager\\n\\nA better way to handle files is to use the context manager (`with` statement). This automatically takes care of closing the file:\\n\\n```python\\nwith open('filename.txt', 'r') as file:\\n    content = file.read()  # Read the file content\\n# No need to call file.close(), it is automatically closed\\n```\\n\\n### Example of File Handling\\n\\nHere's an example that covers opening a file, writing to it, reading from it, and then closing it:\\n\\n```python\\n# Writing to a file\\nwith open('example.txt', 'w') as file:\\n    file.write('Hello, World!\\\\n')\\n    file.write('This is a file handling example.\\\\n')\\n\\n# Reading from a file\\nwith open('example.txt', 'r') as file:\\n    content = file.read()\\n    print(content)\\n```\\n\\n### Summary\\n\\n1. Use `open()` to open a file with specific mode.\\n2. Use methods like `read()`, `readline()`, or `writelines()` for reading/writing.\\n3. Always close the file with `file.close()` or use the `with` statement for automatic handling.\\n4. Be aware of file modes (`r`, `w`, `a`, etc.) to perform the desired operation correctly.\\n\\nThis is how file handling is generally done in Python! If you have more specific questions or need examples, feel free to ask!\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736751292, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=740, prompt_tokens=34, total_tokens=774, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"coding-assistant\",include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'coding-assistant', 'lc_hub_commit_hash': '3351ed5c3b03d705b4a83a906ae712e422f8909b234cc61899df4cea5d0d0425'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You Are Coding expert of {language} language, answer all questions of that language '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'Answers', 'description': \"Extract answers from the LLM's response.\", 'type': 'object', 'properties': {'Answer': {'type': 'string', 'description': 'answer from llm'}}, 'required': ['Answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000022DE56C4CB0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000022DE56C60F0>, root_client=<openai.OpenAI object at 0x0000022DE568B7A0>, root_async_client=<openai.AsyncOpenAI object at 0x0000022DE56C4CE0>, model_name='gpt-4o-mini', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), presence_penalty=0.0, frequency_penalty=0.0, top_p=1.0), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Answers', 'description': \"Extract answers from the LLM's response.\", 'parameters': {'type': 'object', 'properties': {'Answer': {'type': 'string', 'description': 'answer from llm'}}, 'required': ['Answer'], 'strict': True, 'additionalProperties': False}}}], 'parallel_tool_calls': False, 'tool_choice': {'type': 'function', 'function': {'name': 'Answers'}}}, config={}, config_factories=[])\n",
       "| JsonOutputKeyToolsParser(first_tool_only=True, key_name='Answers')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer': \"File handling in Python involves opening, reading, writing, and closing files. Python provides built-in functions to work with files. Below are steps and examples to handle files in Python:\\n\\n### 1. Opening a File\\nYou can open a file using the built-in `open()` function. It requires at least one argument, the filename, and an optional second argument that specifies the mode:\\n- `'r'`: Read (default mode)\\n- `'w'`: Write (overwrites the file or creates a new one)\\n- `'a'`: Append (adds to the end of the file)\\n- `'b'`: Binary mode\\n- `'x'`: Exclusive creation (fails if the file exists)\\n\\n**Example:**\\n```python\\nfile = open('example.txt', 'r')  # Opens a file in read mode\\n```\\n\\n### 2. Reading from a File\\nYou can use several methods to read from a file:\\n- `read(size)`: Reads the specified number of bytes. If no size is specified, it reads the entire file.\\n- `readline()`: Reads a single line from the file.\\n- `readlines()`: Reads all lines into a list.\\n\\n**Example:**\\n```python\\ncontent = file.read()     # Reads the entire content\\nline = file.readline()     # Reads one line\\nlines = file.readlines()   # Reads all lines into a list\\n```\\n\\n### 3. Writing to a File\\nYou can write to a file using the `write()` or `writelines()` methods. Before writing, ensure the file is opened in write or append mode.\\n\\n**Example:**\\n```python\\nfile = open('example.txt', 'w')  # Opens a file in write mode\\nfile.write('Hello, world!')        # Writes a string to the file\\nfile.writelines(['Line 1\\n', 'Line 2\\n'])  # Writes a list of strings\\n```\\n\\n### 4. Closing a File\\nIt is important to close the file after you're done to free up the resources. Use the `close()` method to do this.\\n\\n**Example:**\\n```python\\nfile.close()  # Closes the file\\n```\\n\\n### 5. Using Context Managers\\nA more efficient way to handle files is to use a `with` statement. This automatically handles closing the file for you, even if an error occurs.\\n\\n**Example:**\\n```python\\nwith open('example.txt', 'r') as file:\\n    content = file.read()  # File is automatically closed after this block\\n```\\n\\n### Summary\\nFile handling in Python is straightforward and involves opening files using `open()`, reading or writing with methods like `read()`, `write()`, and closing files with `close()`. Using context managers with `with` is recommended for better practice.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})\n",
    "prompt.invoke({\"question\": \"How We do file handling in python ??\", \"language\": \"Python\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"coding-assistant:7119b04f\",include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\Study\\LangSmith\\ls-academy\\Lib\\site-packages\\langsmith\\client.py:7166\u001b[0m, in \u001b[0;36mconvert_prompt_to_openai_format\u001b[1;34m(messages, model_kwargs)\u001b[0m\n\u001b[0;32m   7165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Study\\LangSmith\\ls-academy\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2045\u001b[0m, in \u001b[0;36mChatOpenAI._get_request_payload\u001b[1;34m(self, input_, stop, **kwargs)\u001b[0m\n\u001b[0;32m   2038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_request_payload\u001b[39m(\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2040\u001b[0m     input_: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2043\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   2044\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m-> 2045\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m     \u001b[38;5;66;03m# max_tokens was deprecated in favor of max_completion_tokens\u001b[39;00m\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;66;03m# in September 2024 release\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\LangSmith\\ls-academy\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:727\u001b[0m, in \u001b[0;36mBaseChatOpenAI._get_request_payload\u001b[1;34m(self, input_, stop, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_request_payload\u001b[39m(\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    722\u001b[0m     input_: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    726\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m--> 727\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Study\\LangSmith\\ls-academy\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:273\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    269\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m )\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLangSmithError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m hydrated_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile handling ??\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m converted_messages \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_prompt_to_openai_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrated_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     11\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m         messages\u001b[38;5;241m=\u001b[39mconverted_messages,\n\u001b[0;32m     13\u001b[0m     )\n",
      "File \u001b[1;32md:\\Study\\LangSmith\\ls-academy\\Lib\\site-packages\\langsmith\\client.py:7168\u001b[0m, in \u001b[0;36mconvert_prompt_to_openai_format\u001b[1;34m(messages, model_kwargs)\u001b[0m\n\u001b[0;32m   7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   7167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 7168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting to OpenAI format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLangSmithError\u001b[0m: Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"File handling ??\", \"language\": \"Python\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer': \"Hey guys! Today, I’ll teach you about file handling in Python. 🐍\\n\\nSo, to start off, you can open a file using the `open()` function. For example, `file = open('myfile.txt', 'r')` opens a file in read mode. You can also use modes like 'w' for write and 'a' for append.\\n\\nOnce you're done, don’t forget to close the file with `file.close()`. But a cool tip is to use the `with` statement: `with open('myfile.txt', 'r') as file:`. This way, the file closes automatically! \\n\\nTo write to a file, you just use `file.write('Hello, World!')`. And to read, you can use `file.read()` or `file.readlines()` for lines. \\n\\nAnd that’s it! Happy coding! 🎉\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"File handling ??\", \"language\": \"Python\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-rag-prompt/75567b82?organizationId=61fc4255-5c94-438a-969e-f7649695f391'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "client.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence/a1948b4e?organizationId=61fc4255-5c94-438a-969e-f7649695f391'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
